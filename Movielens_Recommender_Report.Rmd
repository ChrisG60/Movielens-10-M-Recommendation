---
title: "Data Science Capstone - Movielens Recommendation Model"
author: "Carlos Y치침ez Santib치침ez"
date: "`r format(Sys.time(), '%B %d, %Y')`"
abstract: "Abstract"
output:
  pdf_document:
    number_sections: yes
    toc: yes
    fig_caption: yes
    includes:
      in_header: header_includes.tex
geometry: margin=1in,headheight=70pt,headsep=0.3in
linkcolor: blue
mainfont: Arial
fontsize: 11pt
always_allow_html: true
---


```{r, setup, include=FALSE}
#### WARNING : Running the notebook from scratch, may take SEVERAL HOURS - Run at your own time.####

# Avoid to run time consuming analysis - comment if you want to run the notebook from scratch

knitr::opts_chunk$set(echo=FALSE,eval=FALSE,message=FALSE, warning=FALSE,tidy=FALSE,fig.align="center")

# Load environment with saved dataset for faster knitting - comment if you want to run the notebook from scratch
# you can download the file from https://drive.google.com/open?id=1yrZ4zMqOIawRjUuLXcJz10YcBM8NnSiS

load("movielens.RData") 

# Load functions created for this excercise
source("hybrid_recommender.R", echo = F, prompt.echo = "", spaced = F)


if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(gtools)) install.packages("gtools", repos = "http://cran.us.r-project.org")
if(!require(ClusterR)) install.packages("ClusterR", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(DiagrammeR)) install.packages("DiagrammeR", repos = "http://cran.us.r-project.org")
if(!require(formatR)) install.packages("formatR", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")

knitr::knit_hooks$set(inline = function(x) { if(!is.numeric(x)){ x }else{ prettyNum(round(x,3), big.mark=" ") } })
target_rmse <- 0.86490

```

```{r Load_Movielens}

## Uncomment the below lines to reload the Movielens database
#movielens_10M<-Movielens_Data_Loader()
#edx <- movielens_10M$edx
#validation <- movielens_10M$validation
#rm(movielens_10M)
```
\newpage
# Introduction
 <!-- introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed -->

This document presents a machine learning model that aim to predict (recommend) movie ratings for particular users of a streaming or review platform. This report is a capstone assignment for HarvardX's Professional Certificate in Data Science, which can be taken at the edX platform. The program is available at   [https://www.edx.org/professional-certificate/harvardx-data-science]( https://www.edx.org/professional-certificate/harvardx-data-science).

The data used in this excercise comes from  the [Movielens 10M Dataset](https://grouplens.org/datasets/movielens/10m/). After using the donwload code provided in the course, the resulting dataframe contains observations with an individual movie rating from a particular user, each with the below attributes:

1. **`r colnames(edx)[1]`** : Unique user identifier
2. **`r colnames(edx)[2]`** : Unique movie identified
3. **`r colnames(edx)[3]`** : Rating given to this movie by the particular user.
4. **`r colnames(edx)[4]`** : Timestamp indicating when the the user submitted the rating.
5. **`r colnames(edx)[5]`** : Title of the film and its release year in brackets. Please note that different movies can have the same name (e.g. remakes), thus `r colnames(edx)[2]` is a better unique identifier.
6. **`r colnames(edx)[6]`** : List of all genres in which this movie can be clasiffied.

```{r Column_Names}
#Used to retrieve column names during report writing
colnames(edx)
```

The goal of this project is to generate a model that can predict a particular movie rating for each user, as close as possible to the actual rating given to each film. In order to assess the model performance, the **Root Square Mean Error (RMSE)** will be calculated for a validation dataset. Training and validation dataset are generated using the code provided in the assignment instructions. In order to aim for a full mark, this report will target for a RMSE lower than **`r sprintf("%3.5f", target_rmse)`**.

The starting point of this project is the model presented in section [33.7 of the course's textbook](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems). From there, the following steps are presented:

1. Analysis of the textbook's model and possible ways to improve it.
2. Improvement to the model via user clustering.
3. Tuning improved model.
4. Evaluation against validation dataset.
5. Conclusion.

The following sections present the above points in  further detail.

\newpage
# Methods and Analysis
<!-- methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach -->

## Preparing the data
As mentioned in the introduction, this reports starts with the model presented in the textbook and then explore options to improve segmenting the users. However before conducting any modelling, the data needs to be cleaned up a little bit and then split into training and testing set.

In terms of data cleaning, three operations have been considered, namely:

* Remove the year from the title and storing in a different column. This may be useful for modelling.
* For the same reasons, convert the timestamp into a year number.
* Finally, a sequential number will be added (*row_id*). This will be create a single unique ID for each record (instead of a userId and movieId combination) and maybe useful to speed up filtering, given the large size of the dataset.

For this purposes, the below function **Tidy_Up** has been created. The code for this function is available on the included R file. Using the below code the train and test sets are created.


```{r Tidy_Up_Data, echo=TRUE}
#tidy up data 
edx_tidy_temp <- Tidy_Up(edx)

#divide Train and Test datasets
set.seed(200, sample.kind="Rounding")
edx_train_test <- Train_Test(edx_tidy_temp)

#remove temporary and original dataset to avoid filling up memory.
rm("edx_tidy_temp","edx")

```

Below, this is a sample of the generated training set (*edx_train_test$train*).

```{r sample_training_dataset, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
edx_train_test$train[1:10,] %>% kable(caption="Training Dataset - Sample",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position","scale_down"))
```

## Textbook Model

Once completed data cleaning and split, the first step is to implement the model presented in the textbook and asses it's results. This model estimates rating by assuming that a particular rating from a particular user can be calculated as a deviation (bias) from the average rating for all movies (the *true* rating). In order to account for cases where movies and users don't have many reviews against them, weighting parametres have been added. This can be expressed by the below equation:

$$\large{predicted \ rating =  \hat{\mu} + b_i + b_u}$$

where $\hat(\mu)$ is the average rating for all movies in the dataset, and $b_i$ and $b_u$ being the movie bias and user bias (respectively), defined as follows:

$$b_i = \frac{1}{\lambda_1 + m}\sum_{j=1}^{m} (rating_j - \hat{\mu})$$
$$b_u = \frac{1}{\lambda_2 + n}\sum_{j=1}^{n} (rating_j - \hat{\mu} - b_{i,j})$$

where
* $rating_j$ :a particular movie rating 
* $m$ : the number of all ratings for a particular movie
* $n$ : the total number of ratings submitted from a particular user (one rating per movie).
* $ b_{i,j}$ : the *movie bias* for a particular movie
* $\lambda_1$ :  weighting parametre for *movie biases*.
* $\lambda_2$ :  weighting paramete for *user biases*.


The calculation of these parametres has been written into the function *General_Biases*. Then, these results are used to create the first set of predictions, setting the tuning parametres to zero. The results are evaluated using the provided *RMSE* function.

```{r Calculate_Biases, echo=TRUE, eval=TRUE}
# create biases

general_biases <- General_Biases(edx_train_test$train)

# join with dataset and predict

data_set <- edx_train_test$test  %>%
  left_join(general_biases$movie_avgs, by = "movieId") %>%
  left_join(general_biases$user_avgs, by = "userId")

first_prediction <- data_set %>%
    mutate(predicted_rating = general_biases$mu_hat +
             b_i + b_u, error = rating - predicted_rating) %>%
    filter(!(predicted_rating == general_biases$mu_hat))

# Calculate RMSE and put it in comparison table
rmse_value <- RMSE(first_prediction$rating,
                   first_prediction$predicted_rating)
compare_RMSE <- tibble(Model=character(),
                       RMSE = double(),
                       "Below Target"=logical())
compare_RMSE <- add_row(compare_RMSE,
                        Model="Textbook",
                        RMSE=rmse_value,
                        "Below Target"=(rmse_value<target_rmse))

# clean up 
general_biases_first <- general_biases
rm(first_prediction,data_set,rmse_value)
```

```{r first_Prediction, eval=TRUE}

compare_RMSE %>% kable(caption="First Prediction",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position"),position = "center")
```

As expected, the RMSE is higher than the target value. After thinking about the possible causes of this, part of this may be related to the fact that this model is based on averages. Although this  is not necessarily a bad approach and works for many cases, it also has some limitations, namely:

* it won't perfom well with *divisive* movies. For example a niche film will obtain 5 stars from the fans, but will be many users which will disklike. This won't be reflected by an average bias (and the user bias may not necessarily compensate this).
* There will be users with extrem ratings (i.e. who like family movies and dislike horror films). The user bias won't reflect this.
* Already mentioned in the textbook, when the number of reviews for a user or a film is low , this model won't predict accurately.

Taking this into account, the *General_Biases* function also calculates the standard deviation of the movie and user biases (named *s_i* and *s_u* respectively). The below charts show their results

```{r Movie_Biases_1, eval=TRUE}
 general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>%
  mutate(s_i=round(s_i,5)) %>% ggplot(aes(s_i)) +
  geom_histogram(binwidth=0.01,color='navy',fill='navy') +
  geom_vline(xintercept=1, linetype="dashed", color = "red",size=0.8) + theme_grey()+
  labs(title="Movie biases deviations",x="s_i", y = "Movies")
```

```{r User_Biases, eval=TRUE}
 general_biases_first$user_avgs %>% filter(!is.na(s_u)) %>%
  mutate(s_u=round(s_u,5)) %>% ggplot(aes(s_u)) +
  geom_histogram(binwidth=0.01,color='navy',fill='navy') +
  geom_vline(xintercept=1, linetype="dashed", color = "red",size=0.8) + theme_grey() +
  labs(title="User biases deviations",x="s_u", y = "Movies")
```

As shown, there is a significant percentage of movie (`r round(mean(general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>% .$s_i >=1)*100,2)` %) and user (`r  round(mean(general_biases_first$user_avgs %>% filter(!is.na(s_u)) %>% .$s_u >=1)*100,2)` %) biases equal or larger than 1. This means there will be many cases where this model with values contributing to a larger RMSE. Although movies/user with lower number of reviews have higher deviations, this is not always the case, as illustrated in the below graph.

```{r deviations_per_reviews, eval=TRUE}
general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>% mutate(s_i=round(s_i,3)) %>% group_by(n_i,s_i) %>%
  summarise(size_i=n()) %>%
  ggplot(aes(x=n_i,y=s_i,size=size_i)) + geom_point(color='navy',fill='navy') +  
  labs(title="Deviation per number of reviews",x="Reviews per movie", y = "s_i", size="Number of movies") + 
  geom_hline(yintercept=1, linetype="dashed", color = "red",size=0.8) +
  theme_grey() + scale_x_continuous(trans='log10') 
```

Perhaps, a way to illustrate this is to look at two films with similar number of reviews and average rating (and close $b_i$) but different standard deviations for their ratings.

```{r Deviation Comparison,eval=TRUE}

# Romeo + Juliet
selected_reviews_1<- edx_train_test$train %>% filter(movieId==1059) 
selected_title_1 <- unique(selected_reviews_1$title)
selected_1 <- selected_reviews_1%>%  ggplot(aes(rating))  + geom_histogram(bins = 10,color='navy',fill='navy') +  labs(title=selected_title_1,x="Rating", y = "Reviews")  + theme_grey()

# The Bourne Supremacy
selected_reviews_2<- edx_train_test$train %>% filter(movieId==8665) 
selected_title_2 <- unique(selected_reviews_2$title)
selected_2 <- selected_reviews_2%>%  ggplot(aes(rating))  + geom_histogram(bins = 10,color='navy',fill='navy') +  labs(title=selected_title_2,x="Rating", y = "Reviews")  + theme_grey()

#plot
grid.arrange(selected_1, selected_2, nrow=2)

#cleanup
rm(selected_reviews_1,selected_title_1,selected_1,selected_reviews_2,selected_title_2,selected_2)

common_users <- nrow(edx_train_test$train %>% filter(movieId %in% c(8665,1059)) %>%  select(userId) %>% unique())
```

Looking at the charts above, it looks like *Romeo+Juliet* has a wider range of opinions, compared to a more concentrated range of ratings in the case of the *Bourne Supremacy*, which in turns means the model won't be as effective, especially for the `r common_users` users who have rated both films and may be in all sections of liking/disliking this particular combination of films, which is not captured in the existing model.

## An Improved Model : User Clustering

All the previous observations notwithstanding, it is still worth pointing that the above model has merit, given its relative simplicity and ease to calculate. In addition, in the textbook, this model shows a good performance with a dataset considerably smaller than 8+ million observations.

Perhaps, 8 million reviews is indeed a large number of reviews and it is possible to generate ratings based on smaller groups. This gives the opportunity to segment the data into smaller groups, where deviations are smaller and therefore have  a lower prediction error.

One approach is to cluster the users according their similar movie tastes (i.e split the ones who like Shakespearean romances from the fans of action thrillers). The way used in this report classify the users based on the genres of the films they have rated.

```{r Create_User_Vector}
user_vector <-User_Vectoriser(edx_train_test$train)
```

As mentioned before, each film has a set of genres they belong (from one to many). Collectively, there are `r nrow(user_vector$genres)` genres, listed below.

```{r genres_list, eval=TRUE}
user_vector$genres %>% kable(caption="Genres in training set, ordered by ocurrences",format = "latex", booktabs = TRUE,format.args = list(decimal.mark = '.', big.mark = " ")) %>%  kable_styling(latex_options = c("HOLD_position"))
```

In order to profile each user, this report has used the genre in two ways:

* The average rating each user gives to each genre, for each film rated (i.e. how much each user likes each genre).
* The percentage of each genre compared to the overall reviews per user (i.e. how much each user watches each genre).

This will result in a *user_vector*, profiling each user in the training set. This algorithm, has been written int the *User_Vectoriser* function, present in the attached R file. The below table shows a sample of the user vectors created by this code.

```{r User_Vector_Sample_1, eval=TRUE}

user_vector$user_averages[1:10,] %>% mutate_if(is.numeric, format, digits=4,nsmall = 0) %>%
  kable(caption="User Vectors - Averages",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))
```
```{r User_Vector_Sample_2, eval=TRUE}
user_vector$user_weights[1:10,]  %>% mutate_if(is.numeric, format, digits=4,nsmall = 0) %>%
  kable(caption="User Vectors - Percentages",format = "latex", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position","scale_down"))
```


The average number of genres per user is `r mean(rowSums(user_vector$user_weights[,2:ncol(user_vector$user_weights)]>0))`. 

```{r Genres_per_USer, eval=TRUE}
tibble(genres=rowSums(user_vector$user_weights[,2:ncol(user_vector$user_weights)]>0)) %>% 
  ggplot(aes(genres)) +
  geom_histogram(bins=20,color='navy',fill='navy') + theme_grey() +
  labs(title="Distribution of Number of Genres per User",x="Genres", y = "Users")

```


Using these vectors, it is possible to use existing clustering algorithms such as k-means (presented in [section 34 of the textbook](https://rafalab.github.io/dsbook/clustering.html)). However after an initial test, **Gaussian Mixture Model clustering** was chosen instead. [This link](https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e) offers and explanation of the algorithm and why can be a better method for to account variances in this particular case. In this excercise, the GMM implementation the [**ClusterR**](https://cran.r-project.org/web/packages/ClusterR/ClusterR.pdf) package is used. The code below shows a sample of how GMM is used in this project.


```{r GMM sample,echo=TRUE}
#First we need to combine weights and averages, and remove the first columns
user_profiles <- user_vector$user_averages %>%
      left_join(user_vector$user_weights, by = "userId")
us <- user_profiles %>% select(-userId) 

## Then we need to specified the number of clusters we want the users to be classified into,eg 5
cluster_n <- 5

###The, we run GMM as per the manual
fit <- GMM(us, cluster_n, dist_mode = "maha_dist", 
           seed_mode = "random_subset", km_iter = 10,
           em_iter = 10, verbose = F)

pr <- predict_GMM(us, fit$centroids, fit$covariance_matrices, fit$weights)
user_profiles$group <- pr$cluster_labels

###Show user profiles

user_profiles
```

Before running the above code, it is worth noticing that:

* The combined averages and weights vectors have 40 dimensions, which is large. In addition, most of these vector don't have a presence across all these dimensions (in this particular training set, none has). Potentially there are some vector without any common dimensions (action&adventure fans opposite documentaries&film-noir followers).
* Although no analysis has been done, it is reasonable to assume that some genres have a big overlap, thus not all dimensions are independent from each other.
* There will be a significant number of vectors(users) defined by the most popular dimensions (genres).
* Taking this into account, using all dimensions may not be the best method of clustering - and it is interesting to assess how this impacts the RMSE.
* In addition, the number of clusters  - which will certainly have an impact on the RMSE - is an input parametre.

Considering all the points above and after some initial tests, this document proposes a clustering algorithm based on GMM which follows the below steps:

1. User vector is created.
2. The number of genres used for clustering (*n*) is set.
3. Using the genre percentages, the relative weight of the *n* most popular genres is calculated.
4. The user vector is filtered to only keep the cases where the previously calculated relative weight meets/exceeds a treshold; This filters out the users for which the selected genres are not relevant.
5. The filtered vector is fed into the GMM functions and users are group into an initial set of clusters.
6. The remaining users are sorted, and filtered by their *n* most popular genres. Again, they are filtered by the predifined treshold and fit into a new set of clusters.
7. Step 6 repeated as many times as desired or until there are either no users left or there is no enough data to cluster them.
8. All remaining users (if any) and classified in a residual group.

<!-- The below chart present a flow diagramme for this algorithm. -->

```{r Clustering Flowchart, message=TRUE, include=FALSE}
grViz("digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']
      tab6 [label = '@@6']
      tab7 [label = '@@7']

      # edge definitions with the node IDs
      tab1 -> tab2 -> tab3 -> tab4 -> tab5 -> tab6 -> tab7;

      }

      [1]: 'Calculate user vectors'
      [2]: 'Determine number of genres to use for clustering'
      [3]: 'Determine threshold percentage for selected genres, filter eligible users'
      [4]: 'Cluster filtered users'
      [5]: 'Repeat process with leftover users, as many times as chosen'
      [6]: 'Attempt to cluster remnant users, put rest in special group'
      [7]: 'Consolidate and generate grouping'
      
      ")
```

This algorithm as been written into the **User_Classifier** function, available in the R file. It is worth noticiing this process has four *arbitrary* parametres that will change the number of resulting clusters and therefore the may have an impact on the RMSE - this will be discussed later in the document.

Once the users have been segmented, it is possible to use the *textbook* model to calculate the average and biases for each of these groups and then predict ratings (the objective of this entire process!). The first part of this step is straightforward (just add the group to the dataset and use this parametre to group prior to summarising) and it has been written into a function called **Group_Biases**.

However, for  predicting algorithm needs to consider that potentially some user weren't not grouped into a meaningful cluster and thus this method makes no sense. Additionally, there maybe situations where a particular (group,movie) was absent from the training data and this method is not valid. To cover those situations, the prediction algorithm needs to steps:

1. A prediction method using group averages and biases for all qualifyng reviews - just like the textbook model but split per group.
2. A *backstop* method for all reviews don't fit the above - for this we can use the 'textbook' method, which is better than just guessing.

Please note that both segments have tuning parametres ($\lambda_{1-4}$).

This prediction algorithm has been written into a function called **Rating_Predicter**.

## Tuning Step 1 - Optimise Clustering.

The first optimisation step consists in finding the optimal clustering parametres to minimise the RMSE. These parametres are:

1. the number of genres (*genres*) to consider in each clustering round.
2. the number of target clusters per round (*cluster_n*).
3. the cutoff value to filter the users to feed into each clustering round (*cutoff*).
4. the number of time clustering will be attempted (*iterations*).

This process has been executed through the below code.

```{r Cluster_Optimisation, echo=TRUE}

#create tibble to record stats

stats <- tibble(clustering=character(),iterations=double(),cutoff=double(),
                genres=double(),cluster_n=double(),RMSE = double(),
                RMSE_1 = double(),RMSE_2 = double(),
                RMSE_3 = double(),
                method_1=double(),method_2=double(),
                method_3=double())

#define ranges to iterate

genres_n<-3:6
cluster_n <- 3:6
cutoff <- c(0.4,0.5,0.6)
iterations <-2:4


#iterate predictions
for(g in iterations){
  for(h in cutoff){
    for (i in genres_n){
      for(j in cluster_n){
 
        #create groups, biases  and predict
        
        user_classification  <- User_Classifier(edx_train_test$train,user_vector,
                                                genres=i,step_cutoff=h,cluster_n=j,
                                                cluster_type="GMM",iterations=g)
        
        biases_by_group <- Group_Biases(edx_train_test$train,user_classification)
        
        results<-Rating_Predicter(edx_train_test$test,user_classification,
                                  biases_by_group,general_biases)
        
        # store results in stats tibble
        stats <- add_row(stats,clustering="GMM",iterations=g,
                         cutoff=h,genres=i,cluster_n=j,
                         RMSE = results$RMSE$overall,
                         RMSE_1=results$RMSE$`1`,RMSE_2=results$RMSE$`2`,
                         RMSE_3=results$RMSE$`3`,
                         method_1=results$distribution_percentage$`1`,
                         method_2=results$distribution_percentage$`2`,
                         method_3=results$distribution_percentage$`3`)
     }
   }
 }
}
#cleanup
clustering_stats <- stats
rm(g,h,i,j,results,stats)

```

This code in turn produces the results represented in the below chart.

```{r RMSE Graph, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
clustering_stats%>% ggplot(aes(x=cluster_n,y=RMSE,color=genres)) + geom_point() + facet_grid(.~iterations) +theme(legend.position="bottom") + labs(title="Model tuning by iterations",x="Clusters per iteration", y = "RMSE")+ geom_hline(yintercept=min(clustering_stats$RMSE), linetype="dashed", color = "red",size=0.5) + theme_grey()
```

The optimal solution has the below parametre, which shows also the resulting overall RMSE plus the RMSE for the clustered (RMSE_1) and non-clusters (RMSE_2) groups, along with the proportion of reviews in each method.

```{r Clustering_Statistics, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}

clustering_stats[which.min(clustering_stats$RMSE),] %>% 
  select(-RMSE_3,method_3) %>%
  kable(caption="Optimal Clustering Parametres",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position","scale_down"))
```

When compared with the *texbook* method, this procedure has better results:

```{r second_Prediction, eval=TRUE}

rmse_value <- min(clustering_stats$RMSE)

compare_RMSE <- add_row(compare_RMSE,
                        Model="Optimal Clusters",
                        RMSE=rmse_value,
                        "Below Target"=(rmse_value<target_rmse))

compare_RMSE %>% kable(caption="First and Second Predictions",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("hold_position"),position = "center")

rm(rmse_value)
```

What are reasons this method has a better performance? Some explanations are:

* In this cases, there are different averages for different groups, having more accurate stating points.
* Also, grouping provides movie biases with smaller deviations, contributing to a smaller RMSE. In this case,  `r mean((biases_by_group$movie_group_avgs %>% filter(!is.na(s_m)))$s_m<1)*100` % of the deviations are lower than 1, compared to `r mean((general_biases_first$movie_avgs %>% filter(!is.na(s_i)))$s_i<1)*100
` % for the *textbook* case. This can be visualised in the below chart:

```{r Movie_Biases_2, eval=TRUE}

h1 <- general_biases_first$movie_avgs %>% filter(!is.na(s_i)) %>%
  mutate(s=round(s_i,5),method="general") %>% select(s,method)
h2 <- biases_by_group$movie_group_avgs %>% filter(!is.na(s_m)) %>%
  mutate(s=round(s_m,5),method="clustering") %>% ungroup() %>% select(s,method)

h <-as_tibble(rbind(h1,h2))

ggplot(h,aes(x=s,y=stat(count / sum(count)))) + 
    geom_histogram(data=subset(h,method == 'general'),fill = "red", alpha = 0.2, label="General") +
    geom_histogram(data=subset(h,method == 'clustering'),fill = "blue", alpha = 0.4, label="Clustering") +
  theme(legend.position="bottom") + labs(title="Movie Biases's Std. Dev. Compared",x="Bias Standard Dev.", y = "Density")+ geom_vline(xintercept=1, linetype="dashed", color = "red",size=0.5) 

rm(h1,h2,h)
```

```{r Optimal_Clusters}

genres <- clustering_stats[which.min(clustering_stats$RMSE),]$genres
cluster_n <- clustering_stats[which.min(clustering_stats$RMSE),]$cluster_n
step_cutoff <- clustering_stats[which.min(clustering_stats$RMSE),]$cutoff
clustering_iterations <- clustering_stats[which.min(clustering_stats$RMSE),]$iterations
cluster_type <- "GMM"

user_classification  <- User_Classifier(edx_train_test$train,user_vector,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)

```

It is worth noticing this solution generates splits the training set into `r length(unique(user_classification$group))` groups of users, with the below characteristics:

```{r user_group_stats}

group_stats<-edx_train_test$train %>% left_join(user_classification, by="userId") %>% group_by(group) %>% summarise(users=length(unique(userId)),movies=length(unique(movieId)))
group_stats

most_popular_movies <- tibble(group=double(),top_movies=character())

for(i in as.vector(group_stats$group)){
  temp <- (edx_train_test$train %>% left_join(user_classification, by="userId") %>%
             group_by(group,movieId, title) %>% summarise(reviews=n()) %>% filter(group==i) %>%
             arrange(-reviews) %>% ungroup()%>% select(-movieId))[1:5,]  %>%
             spread(title, reviews) %>% select(-group) %>% colnames(.)
  temp <- paste(temp, collapse = '\n ')
  most_popular_movies<-add_row(most_popular_movies,group=i,top_movies=temp)
}

user_groups_stats <- group_stats %>% left_join(most_popular_movies, by="group")

rm(temp, most_popular_movies, group_stats)
```

```{r stats_table, eval=TRUE}
user_groups_stats %>% kable(caption="Optimal User Groups",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("HOLD_position","scale_down"))

```

Looking at this and the compared deviation histogram's, it looks like there are groups that maybe quite similar  and there improvement in accuracy seems smaller, yet the result improves. Although not covered in there report, perhaps exploring better clustering mechanisms will deliver significantly improved results.

## Tuning $\lambda$ parametres

Having settled on a set of optimal clustering parametres, there is stil room to attempt improving the RMSE but tuning the $\lambda$ parametres in the movie and user bias equations:

$$b_i = \frac{1}{\lambda_1 + m}\sum_{j=1}^{m} (rating_j - \hat{\mu})$$
$$b_u = \frac{1}{\lambda_2 + n}\sum_{j=1}^{n} (rating_j - \hat{\mu} - b_{i,j})$$

In the proposed model, the are four lambda parametres: two for the clustering biases (from now on, $\lambda_1$ and $\lambda_2$) and two for the backstop model ($\lambda_3$ and $\lambda_4$). However, given the previous result shows that over 99% of the dataset fit in the cluster method, thus no tuning of ($\lambda_3$ and $\lambda_4$) will be done at this stage.

In addition, at this stage it would be good to see how much the RMSE naturally varies due to being a random variable - in order to have a level of confidence how well the method will perform against the validation set. For these two reasons, k-fold cross validation will be also done at this point.

There is one problem with the method : since there are three variables to tune and each iteration requires data partition, user clustering, calculating biases and predictions. As experienced in the previous optimisation, this will take **several** hours to complete. As a workaround, tuning will proceed the following way:

1. First, we will run iterations to run $lambda_1$ and $lambda_2$.
3. From this first set, we will choose the twenty combinations with the lowest RMSE.
4. Then, we will run k-fold validation for those twenty combinations.
5. Finally, the optimal set of $lambda_1$ and $lambda_2$ will be chosen picking the lowest average RMSE (from all k iterations).

The first iteration will be implemented with the below code:

```{r Lambda_Tuning_1, echo=TRUE}

stats <- tibble(k=double(),lambda_1=double(),lambda_2=double(),
                RMSE = double(),RMSE_1 = double(),RMSE_2 = double(),
                RMSE_3 =double(),
                method_1=double(),method_2=double(),method_3=double())

lambda_1 <- seq(0,8,0.5)
lambda_2 <- seq(0,8,0.5)


for(k in 1:1){

  edx_cross_validation <- Train_Test(edx_train_test$train)
              
  user_vector <-User_Vectoriser(edx_cross_validation$train)
  general_biases <- General_Biases(edx_cross_validation$train)
  user_classification  <- User_Classifier(edx_cross_validation$train,user_vector,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)  
  
  
  for (i in lambda_1){
    for(j in lambda_2){

              
              biases_by_group<-Group_Biases(edx_cross_validation$train,
                                            user_classification,
                                            lambda_1=i,lambda_2=j)
           
              results<-Rating_Predicter(edx_cross_validation$test,
                                        user_classification,
                                        biases_by_group,general_biases)
              
              stats <- add_row(stats, k=k, lambda_1=i,lambda_2=j,
                               RMSE = results$RMSE$overall,
                         RMSE_1=results$RMSE$`1`,
                         RMSE_2=results$RMSE$`2`,
                         RMSE_3=results$RMSE$`3`,
                         method_1=results$distribution_percentage$`1`,
                         method_2=results$distribution_percentage$`2`,
                         method_3=results$distribution_percentage$`3`)
  }
  }
  
  rm(edx_cross_validation)

}

group_lambda_stats_1 <- stats
rm(i,j,k)
```

The 20 lowest RMSE combinations are presented in the below table:

```{r Lambda_Tuning_2,eval=TRUE}

cases <- 20
group_lambda_stats_1<- unique(group_lambda_stats_1)

#lambda_aggregate_stats_1<-group_lambda_stats_1 %>% group_by(lambda_1,lambda_2) %>% summarise(Avg_RMSE=mean(RMSE),Avg_Method_1=mean(method_1))

lambda_top_20_rmse_1<-group_lambda_stats_1 %>% arrange(RMSE) %>% head(cases)    
lambda_top_20_rmse_1$cases <- seq.int(nrow(lambda_top_20_rmse_1))

lambda_top_20_rmse_1 %>% select(-k,-RMSE_3,-method_3,-cases) %>%
  kable(caption="Lambda Optimisation - Step 1",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))


```

Then, using a modified version of the previous code, 5 more iterations of those 20 sets of $lambdas$ are generated and average RMSEs are calculated. The best 5 results are presented in the below table.

```{r Lambda_Tuning_3}

stats <- tibble(k=double(), cases=double(),lambda_1=double(),lambda_2=double(),
                RMSE = double(),RMSE_1 = double(),RMSE_2 = double(),
                RMSE_3 =double(),
                method_1=double(),method_2=double(),method_3=double())

lambda_1 <- seq(0,8,0.5)
lambda_2 <- seq(0,8,0.5)

for(k in 1:6){

  edx_cross_validation <- Train_Test(edx_train_test$train)
              
  user_vector <-User_Vectoriser(edx_cross_validation$train)
  general_biases <- General_Biases(edx_cross_validation$train)
  user_classification  <- User_Classifier(edx_cross_validation$train,user_vector,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)  
  
  
  for (i in 1:cases){
  

              
              biases_by_group <- Group_Biases(edx_cross_validation$train,user_classification,
                                              lambda_1=lambda_top_20_rmse_1[i,]$lambda_1,
                                              lambda_2=lambda_top_20_rmse_1[i,]$lambda_2)
           
              results<-Rating_Predicter(edx_cross_validation$test,user_classification,biases_by_group,general_biases)
              
              stats <- add_row(stats, k=k, cases=i,lambda_1=lambda_top_20_rmse_1[i,]$lambda_1,
                               lambda_2=lambda_top_20_rmse_1[i,]$lambda_2,
                               RMSE = results$RMSE$overall,
                         RMSE_1=results$RMSE$`1`,RMSE_2=results$RMSE$`2`,RMSE_3=results$RMSE$`3`,
                         method_1=results$distribution_percentage$`1`,method_2=results$distribution_percentage$`2`,
                         method_3=results$distribution_percentage$`3`)
  }
  
  
  rm(edx_cross_validation)

}

group_lambda_stats <- stats
rm(i,j,k)
   
```

```{r Lambda_Tuning_4, eval=TRUE}

group_lambda_stats <-rbind(group_lambda_stats,lambda_top_20_rmse_1)
group_lambda_stats<- unique(group_lambda_stats)

lambda_aggregate_stats<- group_lambda_stats %>% group_by(cases,lambda_1,lambda_2) %>% summarise(Avg_RMSE=mean(RMSE)) %>%ungroup()

lambda_top_rmse<-lambda_aggregate_stats %>% arrange(Avg_RMSE) %>% head(5)    
             
lambda_top_rmse  %>% select(-cases) %>%
  kable(caption="Lambda Optisimation - Step 2",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position"))

```

Using these parametres, the model is retrained using the whole training set and RMSE is calculated against the test set for comparison with the previous two cases.

```{r Retrain_Optimal_Lambdas}
lambda_1<-lambda_top_rmse[which.min(lambda_top_rmse$Avg_RMSE),]$lambda_1
lambda_2<-lambda_top_rmse[which.min(lambda_top_rmse$Avg_RMSE),]$lambda_2

user_vector <-User_Vectoriser(edx_train_test$train)
general_biases <- General_Biases(edx_train_test$train)
user_classification  <- User_Classifier(edx_train_test$train,user_vector,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)
biases_by_group <- Group_Biases(edx_train_test$train,user_classification,lambda_1 = lambda_1,lambda_2 = lambda_2)
results<-Rating_Predicter(edx_train_test$test,user_classification,biases_by_group,general_biases)

stats <- tibble(lambda_1=lambda_1,
                lambda_2=lambda_2,
                RMSE = results$RMSE$overall,
                RMSE_1=results$RMSE$`1`,
                RMSE_2=results$RMSE$`2`,
                method_1=results$distribution_percentage$`1`,
                method_2=results$distribution_percentage$`2`)

optimal_lambdas <- stats
results_test <- results
rm(results,stats)
```

```{r Assess_Test_Set, eval=TRUE }

optimal_lambdas %>%
  kable(caption="Result with Optimal Lambdas on Test Set",format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))
          
```

```{r third_Prediction, eval=TRUE}

rmse_value <- min(optimal_lambdas$RMSE)

compare_RMSE <- add_row(compare_RMSE,
                        Model="Optimal Clusters + Optimised Lambdas",
                        RMSE=rmse_value,
                        "Below Target"=(rmse_value<target_rmse))

compare_RMSE %>% kable(caption="First and Second Predictions",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("HOLD_position"),position = "center")

rm(rmse_value)
```

## Checks prior to prediction

Before moving to assess the model against the validation set, there are two points to inspect. The first point was mentioned before and is the RMSE variance when k-fold validation was done. Taking the 5 best results, it is possible to build the below boxplot. This graph show that all combinations are well below the RMSE threshold!

```{r RMSE_variation}
group_lambda_stats %>% filter(cases %in% lambda_top_rmse$cases) %>% 
  mutate(l1l2=paste(lambda_1,lambda_2,sep=", ")) %>%
  select(l1l2,RMSE) %>%
  ggplot(aes(x=l1l2,y=RMSE,color=l1l2)) + geom_boxplot() + theme(legend.position="bottom") + labs(title="RMSE variation due k-fold validation",x="lambda_1, lambda2", y = "RMSE")+ geom_hline(yintercept=target_rmse, linetype="dashed", color = "red",size=0.5) + theme_grey()
```

The second inspection is to have a look at the films that couldn't be rated by the clustering mechanism. The table below shows the top 20 films in that category. After reviewing this list, those films seem to be mostly obscure and seems reasonable that couldn't be found in the clustering movie biases. Even if the user is not new, this result would be expected for rare films, which are not going to have many reviews. This is also reassuring since it minimises the risk of popular movies falling into this category and risking a higher prediction error.

```{r Analysis_2, eval=TRUE}
### Analyse test results, comments


(results_test$prediction %>% filter(method==2) %>% 
   group_by(title) %>% summarise(n=n()) %>% arrange(-n))[1:20,] %>%
  kable(caption="Non Clusterable Reviews - Movies",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("HOLD_position"),position = "center")


```

# Results
<!-- a results section that presents the modeling results and discusses the model performance -->

With all optimsed paramatres, it is time now to try to predict the ratings for the validation set. This will be done in two parts:

* First, the model will be re-train using all optimal parametres and the whole combined training+test dataset.
* Then the model will be used to predict the ratings for the validation set and the RMSE will be calculated.

The code to achieve this is presented below

```{r Validation_Prediction, echo=TRUE}
# Prediction against validation dataset

##re-join training test tests
edx <- rbind(edx_train_test$train,edx_train_test$test)

### Retrain : reclustering and re-calculation of biases with new dataset

user_vector_prediction <-User_Vectoriser(edx)
general_biases_prediction <- General_Biases(edx)
user_classification_prediction  <- User_Classifier(edx,user_vector_prediction,
                                              genres=genres,step_cutoff=step_cutoff,
                                        cluster_n=cluster_n,cluster_type=cluster_type,
                                        iterations=clustering_iterations)
biases_by_group_prediction <- Group_Biases(edx,
                                           user_classification_prediction,
                                           lambda_1=lambda_1,
                                           lambda_2=lambda_2)

### tidy up validation table

validation<-Tidy_Up(validation)

# Predict ratings

prediction <- Rating_Predicter(validation,
                               user_classification_prediction,
                               biases_by_group_prediction,
                               general_biases_prediction)


##Calcuate method split and RMSE

stats <- tibble(RMSE = double(),RMSE_1 = double(),
                RMSE_2 = double(),
                method_1=double(),method_2=double())

stats <- tibble(RMSE = prediction$RMSE$overall,
                         RMSE_1=prediction$RMSE$`1`,
                         RMSE_2=prediction$RMSE$`2`,
                         method_1=prediction$distribution_percentage$`1`,
                         method_2=prediction$distribution_percentage$`2`)
prediction_stats <-stats

rm("stats")         
```

The RMSE results are shown in the following table:

```{r Prediction_Stats, eval=TRUE}
prediction_stats %>% select(-RMSE_3,-method_3) %>%
  kable(caption="RMSE for validation set",format = "latex", booktabs = TRUE) %>%  kable_styling(latex_options = c("HOLD_position"),position = "center")
```

\newpage
# Conclusion
<!-- a conclusion section that gives a brief summary of the report, its limitations and future work -->


```{r heatmap}

group_vector_averages <-user_vector$user_averages %>% 
  left_join(user_classification,by="userId") %>% select(-userId) %>%
  gather("genre","value",-group) %>% group_by(group,genre) %>% summarise(value=mean(value))

 ggplot(group_vector_averages, aes(as.factor(group), genre,fill=value)) + geom_tile() +  geom_tile() +
  scale_fill_distiller(palette = "RdPu") 

group_vector_weights <-user_vector$user_weights %>% 
  left_join(user_classification,by="userId") %>% select(-userId) %>%
  gather("genre","value",-group) %>% group_by(group,genre) %>% summarise(value=mean(value))

ggplot(group_vector_weights, aes(as.factor(group), genre, fill=value)) + geom_tile() +  geom_tile() +
  scale_fill_distiller(palette = "RdPu") 



```


